{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gc_MsFUSlZfg"
      },
      "outputs": [],
      "source": [
        "# # local csv loading\n",
        "\n",
        "# import pandas as pd\n",
        "# #import mysql.connector\n",
        "# #from mysql.connector import Error\n",
        "\n",
        "# print(\"loading data\")\n",
        "\n",
        "# # Load data from CSV\n",
        "# df = pd.read_csv('heart_disease.csv', nrows=899)\n",
        "# df.rename(columns={'ekgday(day': 'ekgday'}, inplace=True)\n",
        "\n",
        "# print(\"data loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Connecting to S3 bucket and loading data"
      ],
      "metadata": {
        "id": "pyHw2u9mhXQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading required package\n",
        "\n",
        "# !pip install pyspark==3.4.0\n",
        "# !pip install boto3\n",
        "# !pip install scrapy\n",
        "\n",
        "import boto3\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "from scrapy import Selector\n",
        "import requests\n",
        "\n",
        "import random as rd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "from pyspark.sql.functions import col, when, isnan, isnull, count, avg, trim, mean, lit, rand\n",
        "from pyspark.sql.functions import sum as spark_sum\n",
        "\n",
        "\n",
        "\n",
        "def read_data(spark: SparkSession) -> DataFrame:\n",
        "    # you need to change the credentials for yourself\n",
        "\n",
        "    bucket_name = 'brennanbensonawsbucket'\n",
        "    object_key = 'data/heart_disease.csv'\n",
        "\n",
        "    s3 = boto3.client('s3',\n",
        "                      aws_access_key_id=\"ASIAYAAO5HRMG2ADFJ7L\",\n",
        "                      aws_secret_access_key=\"QpFo4m94dxap5OttjkI2w43iIeo9ta0xL05uOf4H\",\n",
        "                      aws_session_token=\"IQoJb3JpZ2luX2VjEBgaCXVzLWVhc3QtMiJHMEUCIQCOdYHSG3VNmaSoWENqnJmqUVauqBWVaablFsbEqnuLDwIgP5vUT/HR1R4ckzjj0pYas2T4DIKwj3DJ+REn5UrKsQUq9AIIkf//////////ARAAGgw1NDk3ODcwOTAwMDgiDN2Eb46CE/3fcHfkwCrIAtO8MKZqpp33DGMuMR0m9+pZdugVANMa+h2FjG9b+OOvjYzC00O9NSpUicz4Y1cttE+vsws3RqoNHAEfO/37Iu/4m2tSk3fNWVEu82LaKKQnOKBelUAwTgvQe8TZ51LAlCGgRDILMrf9Unov/nZT/U/xLfIRm0bZeiOhAtX2N5k1Lepcn/sGzlvBLdu4j9zSpt6mKYeXU68dYvnvZ3EnDdeCjrBYJiKTgoeXilAMf7mwJmg2XwAp225Hyw6nBlaJ2/j8xxxBpPzK7eHi9DIUQ/kf0hCvZOXFyduonER8HAROqpkoqNikBCL/MGcvmaqM0sVGPu1KMI5xRkW07YlLacspU+DRc1ZfWYt+rMzQUxhNJrWdztiRZ39CiL0fpKZhJFTz0kjxHpsQ8d8C5lwaY+GarTNY8+1DFvGkXyuVyT7U037wRAT0/aswqvDCsgY6pwGxmWcE6TcIvkax7vmRkOz08eM1fzKjwb8UL65ghGCkJsMokb7hRWN37tvYlR/sjjs1wYM8YpVRGbRq2RcgQ6Fy5IPmkJ3QzF0cl575gE7R3oxw+QOcX4nsHn6TF+lBCFYaZ/uZOhyImstjXlKAr6lM0y/eyR0mEdzMYqWS09BFGI3U2VnBXevpoOBRl4m0gXRNwyUGkGN8XzD3s9t3tkgL/sxOTWju2g==\"\n",
        "                      )\n",
        "\n",
        "    local_file_path = 'heart_disease.csv'  # Local path where you want to save the file\n",
        "\n",
        "    # Download the file from S3\n",
        "    try:\n",
        "        s3.download_file(bucket_name, object_key, local_file_path)\n",
        "        print(f\"File downloaded successfully to {local_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "\n",
        "    # csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
        "    # body = csv_obj['Body']\n",
        "    # csv_string = body.read().decode('utf-8')\n",
        "\n",
        "    # df = pd.read_csv(BytesIO(csv_string.encode()), nrows=899)\n",
        "    # print(\"loaded csv into df\")\n",
        "    # print(df.head(20))\n",
        "\n",
        "\n",
        "    #only retain the following columns:\n",
        "    # ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs', 'prop', 'nitr', 'pro', 'diuretic',\n",
        "    # 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
        "\n",
        "    # Define the schema for the dataset\n",
        "    schema = StructType([\n",
        "        StructField(\"age\", IntegerType(), True),\n",
        "        StructField(\"sex\", IntegerType(), True),\n",
        "        StructField(\"painloc\", IntegerType(), True),\n",
        "        StructField(\"painexer\", IntegerType(), True),\n",
        "        StructField(\"relrest\", IntegerType(), True),\n",
        "        StructField(\"pncaden\", IntegerType(), True),\n",
        "        StructField(\"cp\", IntegerType(), True),\n",
        "        StructField(\"trestbps\", IntegerType(), True),\n",
        "        StructField(\"htn\", IntegerType(), True),\n",
        "        StructField(\"chol\", IntegerType(), True),\n",
        "        StructField(\"smoke\", IntegerType(), True),\n",
        "        StructField(\"cigs\", IntegerType(), True),\n",
        "        StructField(\"years\", IntegerType(), True),\n",
        "        StructField(\"fbs\", IntegerType(), True),\n",
        "        StructField(\"dm\", IntegerType(), True),\n",
        "        StructField(\"famhist\", IntegerType(), True),\n",
        "        StructField(\"restecg\", IntegerType(), True),\n",
        "        StructField(\"ekgmo\", IntegerType(), True),\n",
        "        StructField(\"ekgday(day\", IntegerType(), True),\n",
        "        StructField(\"ekgyr\", IntegerType(), True),\n",
        "        StructField(\"dig\", IntegerType(), True),\n",
        "        StructField(\"prop\", IntegerType(), True),\n",
        "        StructField(\"nitr\", IntegerType(), True),\n",
        "        StructField(\"pro\", IntegerType(), True),\n",
        "        StructField(\"diuretic\", IntegerType(), True),\n",
        "        StructField(\"proto\", IntegerType(), True),\n",
        "        StructField(\"thaldur\", FloatType(), True),\n",
        "        StructField(\"thaltime\", IntegerType(), True),\n",
        "        StructField(\"met\", IntegerType(), True),\n",
        "        StructField(\"thalach\", IntegerType(), True),\n",
        "        StructField(\"thalrest\", IntegerType(), True),\n",
        "        StructField(\"tpeakbps\", IntegerType(), True),\n",
        "        StructField(\"tpeakbpd\", IntegerType(), True),\n",
        "        StructField(\"dummy\", IntegerType(), True),\n",
        "        StructField(\"trestbpd\", IntegerType(), True),\n",
        "        StructField(\"exang\", IntegerType(), True),\n",
        "        StructField(\"xhypo\", IntegerType(), True),\n",
        "        StructField(\"oldpeak\", FloatType(), True),\n",
        "        StructField(\"slope\", IntegerType(), True),\n",
        "        StructField(\"rldv5\", IntegerType(), True),\n",
        "        StructField(\"rldv5e\", IntegerType(), True),\n",
        "        StructField(\"ca\", IntegerType(), True),\n",
        "        StructField(\"restckm\", IntegerType(), True),\n",
        "        StructField(\"exerckm\", IntegerType(), True),\n",
        "        StructField(\"restef\", IntegerType(), True),\n",
        "        StructField(\"restwm\", IntegerType(), True),\n",
        "        StructField(\"exeref\", IntegerType(), True),\n",
        "        StructField(\"exerwm\", IntegerType(), True),\n",
        "        StructField(\"thal\", IntegerType(), True),\n",
        "        StructField(\"thalsev\", IntegerType(), True),\n",
        "        StructField(\"thalpul\", IntegerType(), True),\n",
        "        StructField(\"earlobe\", IntegerType(), True),\n",
        "        StructField(\"cmo\", IntegerType(), True),\n",
        "        StructField(\"cday\", IntegerType(), True),\n",
        "        StructField(\"cyr\", IntegerType(), True),\n",
        "        StructField(\"target\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "    # Read the dataset\n",
        "    data = spark.read \\\n",
        "        .schema(schema) \\\n",
        "        .option(\"header\", \"false\") \\\n",
        "        .option(\"inferSchema\", \"false\") \\\n",
        "        .csv(\"heart_disease.csv\")\n",
        "\n",
        "    data = data.limit(900) # limit to first 900 rows\n",
        "\n",
        "    # Convert columns to appropriate types as per schema\n",
        "    for field in schema.fields:\n",
        "        data = data.withColumn(field.name, col(field.name).cast(field.dataType))\n",
        "\n",
        "    # Show the first 5 rows of the dataset\n",
        "    data.show(5)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "####################################### main #######################################\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read heart disease dataset\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = read_data(spark)\n",
        "\n",
        "# Check the schema\n",
        "df.printSchema()\n",
        "\n",
        "# Count null values in 'trestbps' to verify\n",
        "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
      ],
      "metadata": {
        "id": "8VXSZUGrH2TF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a8d955-8e42-4c29-bd16-9bbbd58dc11c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully to heart_disease.csv\n",
            "+----+----+-------+--------+-------+-------+----+--------+----+----+-----+----+-----+----+----+-------+-------+-----+----------+-----+----+----+----+----+--------+-----+-------+--------+----+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+----+-------+-------+------+------+------+------+----+-------+-------+-------+----+----+----+------+\n",
            "| age| sex|painloc|painexer|relrest|pncaden|  cp|trestbps| htn|chol|smoke|cigs|years| fbs|  dm|famhist|restecg|ekgmo|ekgday(day|ekgyr| dig|prop|nitr| pro|diuretic|proto|thaldur|thaltime| met|thalach|thalrest|tpeakbps|tpeakbpd|dummy|trestbpd|exang|xhypo|oldpeak|slope|rldv5|rldv5e|  ca|restckm|exerckm|restef|restwm|exeref|exerwm|thal|thalsev|thalpul|earlobe| cmo|cday| cyr|target|\n",
            "+----+----+-------+--------+-------+-------+----+--------+----+----+-----+----+-----+----+----+-------+-------+-----+----------+-----+----+----+----+----+--------+-----+-------+--------+----+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+----+-------+-------+------+------+------+------+----+-------+-------+-------+----+----+----+------+\n",
            "|null|null|   null|    null|   null|   null|null|    null|null|null| null|null| null|null|null|   null|   null| null|      null| null|null|null|null|null|    null| null|   null|    null|null|   null|    null|    null|    null| null|    null| null| null|   null| null| null|  null|null|   null|   null|  null|  null|  null|  null|null|   null|   null|   null|null|null|null|  null|\n",
            "|  63|   1|   null|    null|   null|   null|   1|     145|   1| 233| null|  50|   20|   1|null|      1|      2|    2|         3|   81|   0|   0|   0|   0|       0|    1|   10.5|       6|  13|    150|      60|     190|      90|  145|      85|    0|    0|    2.3|    3| null|   172|   0|   null|   null|  null|  null|  null|  null|   6|   null|   null|   null|   2|  16|  81|     0|\n",
            "|  67|   1|   null|    null|   null|   null|   4|     160|   1| 286| null|  40|   40|   0|null|      1|      2|    3|         5|   81|   0|   1|   0|   0|       0|    1|    9.5|       6|  13|    108|      64|     160|      90|  160|      90|    1|    0|    1.5|    2| null|   185|   3|   null|   null|  null|  null|  null|  null|   3|   null|   null|   null|   2|   5|  81|     1|\n",
            "|  67|   1|   null|    null|   null|   null|   4|     120|   1| 229| null|  20|   35|   0|null|      1|      2|    2|        19|   81|   0|   1|   0|   0|       0|    1|    8.5|       6|  10|    129|      78|     140|      80|  120|      80|    1|    0|    2.6|    2| null|   150|   2|   null|   null|  null|  null|  null|  null|   7|   null|   null|   null|   2|  20|  81|     1|\n",
            "|  37|   1|   null|    null|   null|   null|   3|     130|   0| 250| null|   0|    0|   0|null|      1|      0|    2|        13|   81|   0|   1|   0|   0|       0|    1|   13.0|      13|  17|    187|      84|     195|      68|  130|      78|    0|    0|    3.5|    3| null|   167|   0|   null|   null|  null|  null|  null|  null|   3|   null|   null|   null|   2|   4|  81|     0|\n",
            "+----+----+-------+--------+-------+-------+----+--------+----+----+-----+----+-----+----+----+-------+-------+-----+----------+-----+----+----+----+----+--------+-----+-------+--------+----+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+----+-------+-------+------+------+------+------+----+-------+-------+-------+----+----+----+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- sex: integer (nullable = true)\n",
            " |-- painloc: integer (nullable = true)\n",
            " |-- painexer: integer (nullable = true)\n",
            " |-- relrest: integer (nullable = true)\n",
            " |-- pncaden: integer (nullable = true)\n",
            " |-- cp: integer (nullable = true)\n",
            " |-- trestbps: integer (nullable = true)\n",
            " |-- htn: integer (nullable = true)\n",
            " |-- chol: integer (nullable = true)\n",
            " |-- smoke: integer (nullable = true)\n",
            " |-- cigs: integer (nullable = true)\n",
            " |-- years: integer (nullable = true)\n",
            " |-- fbs: integer (nullable = true)\n",
            " |-- dm: integer (nullable = true)\n",
            " |-- famhist: integer (nullable = true)\n",
            " |-- restecg: integer (nullable = true)\n",
            " |-- ekgmo: integer (nullable = true)\n",
            " |-- ekgday(day: integer (nullable = true)\n",
            " |-- ekgyr: integer (nullable = true)\n",
            " |-- dig: integer (nullable = true)\n",
            " |-- prop: integer (nullable = true)\n",
            " |-- nitr: integer (nullable = true)\n",
            " |-- pro: integer (nullable = true)\n",
            " |-- diuretic: integer (nullable = true)\n",
            " |-- proto: integer (nullable = true)\n",
            " |-- thaldur: float (nullable = true)\n",
            " |-- thaltime: integer (nullable = true)\n",
            " |-- met: integer (nullable = true)\n",
            " |-- thalach: integer (nullable = true)\n",
            " |-- thalrest: integer (nullable = true)\n",
            " |-- tpeakbps: integer (nullable = true)\n",
            " |-- tpeakbpd: integer (nullable = true)\n",
            " |-- dummy: integer (nullable = true)\n",
            " |-- trestbpd: integer (nullable = true)\n",
            " |-- exang: integer (nullable = true)\n",
            " |-- xhypo: integer (nullable = true)\n",
            " |-- oldpeak: float (nullable = true)\n",
            " |-- slope: integer (nullable = true)\n",
            " |-- rldv5: integer (nullable = true)\n",
            " |-- rldv5e: integer (nullable = true)\n",
            " |-- ca: integer (nullable = true)\n",
            " |-- restckm: integer (nullable = true)\n",
            " |-- exerckm: integer (nullable = true)\n",
            " |-- restef: integer (nullable = true)\n",
            " |-- restwm: integer (nullable = true)\n",
            " |-- exeref: integer (nullable = true)\n",
            " |-- exerwm: integer (nullable = true)\n",
            " |-- thal: integer (nullable = true)\n",
            " |-- thalsev: integer (nullable = true)\n",
            " |-- thalpul: integer (nullable = true)\n",
            " |-- earlobe: integer (nullable = true)\n",
            " |-- cmo: integer (nullable = true)\n",
            " |-- cday: integer (nullable = true)\n",
            " |-- cyr: integer (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            "\n",
            "+---+---+-------+--------+-------+-------+---+--------+---+----+-----+----+-----+---+---+-------+-------+-----+----------+-----+---+----+----+---+--------+-----+-------+--------+---+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+---+-------+-------+------+------+------+------+----+-------+-------+-------+---+----+---+------+\n",
            "|age|sex|painloc|painexer|relrest|pncaden| cp|trestbps|htn|chol|smoke|cigs|years|fbs| dm|famhist|restecg|ekgmo|ekgday(day|ekgyr|dig|prop|nitr|pro|diuretic|proto|thaldur|thaltime|met|thalach|thalrest|tpeakbps|tpeakbpd|dummy|trestbpd|exang|xhypo|oldpeak|slope|rldv5|rldv5e| ca|restckm|exerckm|restef|restwm|exeref|exerwm|thal|thalsev|thalpul|earlobe|cmo|cday|cyr|target|\n",
            "+---+---+-------+--------+-------+-------+---+--------+---+----+-----+----+-----+---+---+-------+-------+-----+----------+-----+---+----+----+---+--------+-----+-------+--------+---+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+---+-------+-------+------+------+------+------+----+-------+-------+-------+---+----+---+------+\n",
            "|  1|  1|    283|     283|    287|    900|  1|      60| 35|  31|  670| 421|  433| 91|805|    423|      3|   54|        55|   54| 69|  67|  66| 64|      83|  113|     57|     552|121|     56|      57|      64|      64|   60|      60|   56|   59|     63|  309|  426|   143|609|    900|    899|   900|   870|   900|   895| 478|    770|    856|    899| 12|  10| 10|     1|\n",
            "+---+---+-------+--------+-------+-------+---+--------+---+----+-----+----+-----+---+---+-------+-------+-----+----------+-----+---+----+----+---+--------+-----+-------+--------+---+-------+--------+--------+--------+-----+--------+-----+-----+-------+-----+-----+------+---+-------+-------+------+------+------+------+----+-------+-------+-------+---+----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Data imputation"
      ],
      "metadata": {
        "id": "moGc5o7qiR0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 2.1: Retain only the necessary columns"
      ],
      "metadata": {
        "id": "339J4fCjUsfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retain = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs', 'prop', 'nitr', 'pro', 'diuretic',\n",
        "          'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
        "df = df.select([col(c) for c in retain])\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvL3wtMQP16M",
        "outputId": "c7dbbf95-8a46-42c5-96f9-58134bf63e17"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+--------+----+--------+-----+----+----+----+----+--------+-------+-------+-----+-------+-----+------+\n",
            "| age| sex|painloc|painexer|  cp|trestbps|smoke| fbs|prop|nitr| pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|\n",
            "+----+----+-------+--------+----+--------+-----+----+----+----+----+--------+-------+-------+-----+-------+-----+------+\n",
            "|null|null|   null|    null|null|    null| null|null|null|null|null|    null|   null|   null| null|   null| null|  null|\n",
            "|  63|   1|   null|    null|   1|     145| null|   1|   0|   0|   0|       0|   10.5|    150|    0|    2.3|    3|     0|\n",
            "|  67|   1|   null|    null|   4|     160| null|   0|   1|   0|   0|       0|    9.5|    108|    1|    1.5|    2|     1|\n",
            "|  67|   1|   null|    null|   4|     120| null|   0|   1|   0|   0|       0|    8.5|    129|    1|    2.6|    2|     1|\n",
            "|  37|   1|   null|    null|   3|     130| null|   0|   1|   0|   0|       0|   13.0|    187|    0|    3.5|    3|     0|\n",
            "|  41|   0|   null|    null|   2|     130| null|   0|   0|   0|   0|       0|    7.0|    172|    0|    1.4|    1|     0|\n",
            "|  56|   1|   null|    null|   2|     120| null|   0|   0|   0|   0|       0|   11.3|    178|    0|    0.8|    1|     0|\n",
            "|  62|   0|   null|    null|   4|     140| null|   0|   0|   0|   0|       0|    6.0|    160|    0|    3.6|    3|     1|\n",
            "|  57|   0|   null|    null|   4|     120| null|   0|   0|   0|   0|       0|    9.0|    163|    1|    0.6|    1|     0|\n",
            "|  63|   1|   null|    null|   4|     130| null|   0|   1|   1|   0|       0|    8.0|    147|    0|    1.4|    2|     1|\n",
            "+----+----+-------+--------+----+--------+-----+----+----+----+----+--------+-------+-------+-----+-------+-----+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 2.2: Impute the remaining columns (besides 'smoke')"
      ],
      "metadata": {
        "id": "iUVPWOitifK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of null values in each column\n",
        "total_count = df.count()\n",
        "null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
        "null_counts = null_counts.collect()[0].asDict()\n",
        "\n",
        "null_percentages = {k: (v / total_count) * 100 for k, v in null_counts.items()}\n",
        "null_percentages_sorted = dict(sorted(null_percentages.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Print each key-value pair in sorted order\n",
        "print(\"Percentage of null values in each column:\")\n",
        "for column, percentage in null_percentages_sorted.items():\n",
        "    print(f\"{column}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nySJz5mcXH1L",
        "outputId": "079effba-bbd4-4342-d125-c2032bb8f597"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of null values in each column:\n",
            "smoke: 74.44%\n",
            "slope: 34.33%\n",
            "painloc: 31.44%\n",
            "painexer: 31.44%\n",
            "fbs: 10.11%\n",
            "diuretic: 9.22%\n",
            "prop: 7.44%\n",
            "nitr: 7.33%\n",
            "pro: 7.11%\n",
            "oldpeak: 7.00%\n",
            "trestbps: 6.67%\n",
            "thaldur: 6.33%\n",
            "thalach: 6.22%\n",
            "exang: 6.22%\n",
            "age: 0.11%\n",
            "sex: 0.11%\n",
            "cp: 0.11%\n",
            "target: 0.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Cleaning and imputing steps for columns other than `smoke:’\n",
        "\n",
        "# drop the rows with all columns NULL.\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "# painloc, painexer: These are binary columns. Impute by replacing null values with the median.\n",
        "binary_cols = ['painloc', 'painexer']\n",
        "for col_name in binary_cols:\n",
        "    median_val = df.approxQuantile(col_name, [0.5], 0.0)[0]\n",
        "    print(\"Imputing null\", col_name, \"rows with median value: \" + str(median_val))\n",
        "    df = df.fillna({col_name: median_val})\n",
        "\n",
        "# tresbps: Replace values less than 100 mm Hg with null and then replace nulls with the mean.\n",
        "df = df.withColumn('trestbps', when(col('trestbps') < 100, None).otherwise(col('trestbps')))\n",
        "mean_trestbps = df.filter(df['trestbps'].isNotNull()).select(mean(col('trestbps'))).collect()[0][0]\n",
        "print(\"Imputing null trestbps rows with mean value: \" + str(mean_trestbps))\n",
        "df = df.fillna({'trestbps': mean_trestbps})\n",
        "\n",
        "# oldpeak: Replace values less than 0 and those greater than 4 with null, then replace nulls with the mean.\n",
        "df = df.withColumn('oldpeak', when((col('oldpeak') < 0) | (col('oldpeak') > 4), None).otherwise(col('oldpeak')))\n",
        "mean_oldpeak = df.filter(df['oldpeak'].isNotNull()).select(mean(col('oldpeak'))).collect()[0][0]\n",
        "print(\"Imputing null oldpeak rows with mean value: \" + str(mean_oldpeak))\n",
        "df = df.fillna({'oldpeak': mean_oldpeak})\n",
        "\n",
        "# thaldur, thalach: Replace the missing values with the mean.\n",
        "continuous_cols = ['thaldur', 'thalach']\n",
        "for col_name in continuous_cols:\n",
        "    mean_val = df.select(mean(col_name)).first()[0]\n",
        "    print(\"Imputing null\", col_name, \"rows with mean value: \" + str(mean_val))\n",
        "    df = df.fillna({col_name: mean_val})\n",
        "\n",
        "# fbs, prop, nitr, pro, diuretic: Replace the missing values and values greater than 1 with the median.\n",
        "binary_cols_with_upper_bound = ['fbs', 'prop', 'nitr', 'pro', 'diuretic']\n",
        "for col_name in binary_cols_with_upper_bound:\n",
        "    df = df.withColumn(col_name, when(col(col_name) > 1, None).otherwise(col(col_name)))\n",
        "    median_val = df.approxQuantile(col_name, [0.5], 0.0)[0]\n",
        "    print(\"Imputing null\", col_name, \"rows with median value: \" + str(median_val))\n",
        "    df = df.fillna({col_name: median_val})\n",
        "\n",
        "# exang, slope: Replace the missing values with the median.\n",
        "categorical_cols = ['exang', 'slope']\n",
        "for col_name in categorical_cols:\n",
        "    median_val = df.approxQuantile(col_name, [0.5], 0.0)[0]\n",
        "    print(\"Imputing null\", col_name, \"rows with median value: \" + str(median_val))\n",
        "    df = df.fillna({col_name: median_val})\n",
        "\n",
        "# Show the cleaned DataFrame\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "qs_l1Mlo4MP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e0c240-dcbf-41ee-d51f-e8953c1f7328"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputing null painloc rows with median value: 1.0\n",
            "Imputing null painexer rows with median value: 1.0\n",
            "Imputing null trestbps rows with mean value: 132.8186215235792\n",
            "Imputing null oldpeak rows with mean value: 0.8732926826197199\n",
            "Imputing null thaldur rows with mean value: 8.655871887648233\n",
            "Imputing null thalach rows with mean value: 137.29857819905214\n",
            "Imputing null fbs rows with median value: 0.0\n",
            "Imputing null prop rows with median value: 0.0\n",
            "Imputing null nitr rows with median value: 0.0\n",
            "Imputing null pro rows with median value: 0.0\n",
            "Imputing null diuretic rows with median value: 0.0\n",
            "Imputing null exang rows with median value: 0.0\n",
            "Imputing null slope rows with median value: 2.0\n",
            "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
            "|age|sex|painloc|painexer| cp|trestbps|smoke|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|\n",
            "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
            "| 63|  1|      1|       1|  1|     145| null|  1|   0|   0|  0|       0|   10.5|    150|    0|    2.3|    3|     0|\n",
            "| 67|  1|      1|       1|  4|     160| null|  0|   1|   0|  0|       0|    9.5|    108|    1|    1.5|    2|     1|\n",
            "| 67|  1|      1|       1|  4|     120| null|  0|   1|   0|  0|       0|    8.5|    129|    1|    2.6|    2|     1|\n",
            "| 37|  1|      1|       1|  3|     130| null|  0|   1|   0|  0|       0|   13.0|    187|    0|    3.5|    3|     0|\n",
            "| 41|  0|      1|       1|  2|     130| null|  0|   0|   0|  0|       0|    7.0|    172|    0|    1.4|    1|     0|\n",
            "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 2.3: Remove rows with null values in 'target' (we should not impute the column we are training to classify)."
      ],
      "metadata": {
        "id": "IzwvYzGwinb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with null values in the 'target' column\n",
        "df = df.dropna(subset=['target'])\n",
        "\n",
        "# Calculate the percentage of null values in each column\n",
        "total_count = df.count()\n",
        "null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
        "null_counts = null_counts.collect()[0].asDict()\n",
        "\n",
        "null_percentages = {k: (v / total_count) * 100 for k, v in null_counts.items()}\n",
        "null_percentages_sorted = dict(sorted(null_percentages.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Print each key-value pair in sorted order\n",
        "print(\"Percentage of null values in each column:\")\n",
        "for column, percentage in null_percentages_sorted.items():\n",
        "    print(f\"{column}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9LtzIH1Kz_s",
        "outputId": "2229ed65-4f78-4e69-a5d1-82c94d41d063"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of null values in each column:\n",
            "smoke: 74.42%\n",
            "age: 0.00%\n",
            "sex: 0.00%\n",
            "painloc: 0.00%\n",
            "painexer: 0.00%\n",
            "cp: 0.00%\n",
            "trestbps: 0.00%\n",
            "fbs: 0.00%\n",
            "prop: 0.00%\n",
            "nitr: 0.00%\n",
            "pro: 0.00%\n",
            "diuretic: 0.00%\n",
            "thaldur: 0.00%\n",
            "thalach: 0.00%\n",
            "exang: 0.00%\n",
            "oldpeak: 0.00%\n",
            "slope: 0.00%\n",
            "target: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section 2.4: Impute the smoke column\n",
        "Retrieve the age/sex-based smoke values from two sources (ABS & CDC), and use those to impute the 'smoke' column.\n",
        "\n",
        "In summary, create two more columns ('abs_smoke' and 'cdc_smoke') that will contain randomly generated values according to the percent chance each person will be a smoker based on their age (in the case of the ABS column) or age and sex (in the case of the CDC column).\n",
        "\n",
        "Then, based on those percentages for each age bucket and sex from these two sources, calculate the average percentage from the two sources for each age bucket and sex, then use those values to impute the 'smoke' column based on each person's age and sex."
      ],
      "metadata": {
        "id": "lyXq2SYiLVDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source the data."
      ],
      "metadata": {
        "id": "_8y5cLu8jv63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Cleaning and imputing the smoke column\n",
        "\n",
        "\n",
        "\n",
        "# You will impute the missing values in the “smoke” column with smoking rates\n",
        "#   by age or sex.\n",
        "# You will use two different sources for obtaining these smoking rates.\n",
        "# Create a separate column for each source.\n",
        "# After imputing the missing values, apply an appropriate transform\n",
        "#   for each column\n",
        "\n",
        "\n",
        "url_1 = \"https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking-and-vaping/latest-release\"\n",
        "# This source lists smoking rates (current daily smokers) by age group.\n",
        "# Replace the missing values with the smoking rate in the corresponding age groups.\n",
        "response = requests.get(url_1)\n",
        "if response.status_code != 200:\n",
        "    print(\"request failed\")\n",
        "\n",
        "html_content = response.content\n",
        "full_sel = Selector(text=html_content)\n",
        "table_selector = full_sel.xpath(\"//table[caption='Proportion of people 15 years and over who were current daily smokers by age, 2011–12 to 2022']/tbody/tr\")\n",
        "\n",
        "age_to_smoking_rate = {}\n",
        "# Iterate through rows\n",
        "for row in table_selector:\n",
        "\n",
        "    rowValAgeRange = row.xpath('.//th[@class=\"row-header\"]/text()').extract()[0]\n",
        "    if \"–\" in rowValAgeRange:\n",
        "        rangeAges = rowValAgeRange.split(\"–\")\n",
        "        ageRange = (int(rangeAges[0]), int(rangeAges[1]))\n",
        "    else:\n",
        "        ageRange = (int(rowValAgeRange[:2]), float('inf')) # first 2 chars = digits\n",
        "    col2022 = row.xpath('.//td[@class=\"data-value\"][position()=last()-2]/text()').extract()[0]\n",
        "\n",
        "    age_to_smoking_rate[ageRange] = float(col2022)\n",
        "\n",
        "print(\"smoking rates by age (abs):\",age_to_smoking_rate)\n",
        "\n",
        "\n",
        "url_2 = \"https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm\"\n",
        "# This source lists smoking rates by age group and by sex.\n",
        "# • For female patients, replace the missing values with the smoking rate in\n",
        "#     their corresponding age groups.\n",
        "# • For male patients, replace the missing values with\n",
        "# smoking rate in age group * (smoking rate among men / smoking rate among women)\n",
        "\n",
        "response = requests.get(url_2)\n",
        "if response.status_code != 200:\n",
        "    print(\"request failed\")\n",
        "\n",
        "html_content = response.content\n",
        "full_sel = Selector(text=html_content)\n",
        "row_selector = full_sel.xpath(\"//div[@class='row '][3]\")\n",
        "\n",
        "ul_selector = row_selector.xpath(\"//ul[@class='block-list']\")\n",
        "\n",
        "sex_li_text = ul_selector[0].xpath(\".//li/text()\") # sex from first row\n",
        "age_li_text = ul_selector[1].xpath(\".//li/text()\") # age from second row\n",
        "\n",
        "malePercent = float(sex_li_text[0].extract().split(\"(\")[1].split(\"%)\")[0])\n",
        "femalePercent = float(sex_li_text[1].extract().split(\"(\")[1].split(\"%)\")[0])\n",
        "\n",
        "ageDict = {}\n",
        "for age_li in age_li_text:\n",
        "    ageRange = age_li.extract().split(\"aged \")[1].split(\" years\")[0]\n",
        "    percentValue = float(age_li.extract().split(\"(\")[1].split(\"%)\")[0])\n",
        "    if \"–\" in ageRange:\n",
        "        ages = ageRange.split(\"–\")\n",
        "        ageDict[(int(ages[0]), int(ages[1]))] = percentValue\n",
        "    else:\n",
        "        ageDict[(int(ageRange), float('inf'))] = percentValue\n",
        "\n",
        "# ageDict contains the flat rates for each age group. This is what we should\n",
        "#   use to impute for female patients, but we need to change the rates for male\n",
        "#   patients according to the formula:\n",
        "#   smoking rate in age group * (smoking rate among men / smoking rate among women)\n",
        "\n",
        "maleSmokingRates = {}\n",
        "for k in ageDict.keys():\n",
        "    maleSmokingRates[k] = ageDict[k] * (malePercent / femalePercent)\n",
        "\n",
        "print(\"male smoking rates by age (cdc):\",maleSmokingRates)\n",
        "print(\"female smoking rates by age (cdc):\",ageDict)"
      ],
      "metadata": {
        "id": "bW_ntS8s-QW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2a55b9-d2b1-47af-e995-e1c7f3a78446"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smoking rates by age (abs): {(15, 17): 1.6, (18, 24): 7.3, (25, 34): 10.9, (35, 44): 10.9, (45, 54): 13.8, (55, 64): 14.9, (65, 74): 8.7, (75, inf): 2.9}\n",
            "male smoking rates by age (cdc): {(18, 24): 6.874257425742575, (25, 44): 16.342574257425742, (45, 64): 19.325742574257426, (65, inf): 10.765346534653467}\n",
            "female smoking rates by age (cdc): {(18, 24): 5.3, (25, 44): 12.6, (45, 64): 14.9, (65, inf): 8.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the 'abs_smoke' and 'cdc_smoke' columns based on the percentages acquired. Randomly generate values based on each person's age/sex."
      ],
      "metadata": {
        "id": "2B2DZXXNjxn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# three cols --> use other two (combination or one of each) to impute the original smoke col\n",
        "# for percents just do random and see if in\n",
        "\n",
        "\n",
        "# Initialize abs_smoke and cdc_smoke columns\n",
        "df = df.withColumn('abs_smoke', lit(0)).withColumn('cdc_smoke', lit(0))\n",
        "\n",
        "\n",
        "\n",
        "# Apply transformations based on the dictionaries\n",
        "for age_range, percent_chance in age_to_smoking_rate.items():\n",
        "    df = df.withColumn(\n",
        "        'abs_smoke',\n",
        "        when(\n",
        "            (col('age') >= age_range[0]) & (col('age') <= age_range[1]) & (rand() <= percent_chance / 100),\n",
        "            1\n",
        "        ).otherwise(col('abs_smoke'))\n",
        "    )\n",
        "\n",
        "for age_range, percent_chance in maleSmokingRates.items():\n",
        "    df = df.withColumn(\n",
        "        'cdc_smoke',\n",
        "        when(\n",
        "            (col('sex') == 1) & (col('age') >= age_range[0]) & (col('age') <= age_range[1]) & (rand() <= percent_chance / 100),\n",
        "            1\n",
        "        ).otherwise(col('cdc_smoke'))\n",
        "    )\n",
        "\n",
        "for age_range, percent_chance in ageDict.items():\n",
        "    df = df.withColumn(\n",
        "        'cdc_smoke',\n",
        "        when(\n",
        "            (col('sex') == 0) & (col('age') >= age_range[0]) & (col('age') <= age_range[1]) & (rand() <= percent_chance / 100),\n",
        "            1\n",
        "        ).otherwise(col('cdc_smoke'))\n",
        "    )\n",
        "\n",
        "# Show the result\n",
        "print(\"After creating the abs_smoke and cdc_smoke columns:\")\n",
        "df.select('age', 'sex', 'smoke', 'abs_smoke', 'cdc_smoke').show(20)\n",
        "\n",
        "# Count the number of 1s in abs_smoke\n",
        "num_abs_smoke_ones = df.select(spark_sum(col('abs_smoke')).alias('num_abs_smoke_ones')).collect()[0]['num_abs_smoke_ones']\n",
        "\n",
        "# Count the number of 1s in cdc_smoke\n",
        "num_cdc_smoke_ones = df.select(spark_sum(col('cdc_smoke')).alias('num_cdc_smoke_ones')).collect()[0]['num_cdc_smoke_ones']\n",
        "\n",
        "print(f\"Number of 1s in abs_smoke: {num_abs_smoke_ones}\")\n",
        "print(f\"Number of 1s in cdc_smoke: {num_cdc_smoke_ones}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwp1-FsAZ1l9",
        "outputId": "08ce2218-47ed-4d20-bbd5-273b820d1ac5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After creating the abs_smoke and cdc_smoke columns:\n",
            "+---+---+-----+---------+---------+\n",
            "|age|sex|smoke|abs_smoke|cdc_smoke|\n",
            "+---+---+-----+---------+---------+\n",
            "| 63|  1| null|        0|        0|\n",
            "| 67|  1| null|        0|        0|\n",
            "| 67|  1| null|        0|        0|\n",
            "| 37|  1| null|        0|        0|\n",
            "| 41|  0| null|        0|        0|\n",
            "| 56|  1| null|        1|        0|\n",
            "| 62|  0| null|        1|        0|\n",
            "| 57|  0| null|        1|        0|\n",
            "| 63|  1| null|        0|        0|\n",
            "| 53|  1| null|        0|        0|\n",
            "| 57|  1| null|        0|        0|\n",
            "| 56|  0| null|        1|        0|\n",
            "| 56|  1| null|        0|        0|\n",
            "| 44|  1| null|        0|        0|\n",
            "| 52|  1| null|        0|        0|\n",
            "| 57|  1| null|        0|        1|\n",
            "| 48|  1| null|        0|        0|\n",
            "| 54|  1| null|        0|        0|\n",
            "| 48|  0| null|        1|        0|\n",
            "| 49|  1| null|        0|        1|\n",
            "+---+---+-----+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Number of 1s in abs_smoke: 141\n",
            "Number of 1s in cdc_smoke: 155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impute the smoke column. Calculate the average percentage between the CDC and ABS sources for each sex and age bucket, and use that to randomly generate values to replace null values in the smoke column."
      ],
      "metadata": {
        "id": "7c2IHr7BksOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now impute the smoke column.\n",
        "\n",
        "# Calculate the percent for male and female\n",
        "smoke_percent_male = {}\n",
        "smoke_percent_female = {}\n",
        "smoke_starts = {}\n",
        "for k in age_to_smoking_rate.keys():\n",
        "    smoke_percent_male[k] = age_to_smoking_rate[k]\n",
        "    smoke_percent_female[k] = age_to_smoking_rate[k]\n",
        "    smoke_starts[k[0]] = k\n",
        "\n",
        "\n",
        "for k in maleSmokingRates.keys():\n",
        "    rangeStart = k[0]\n",
        "    rangeEnd = k[1]\n",
        "    if rangeEnd == float('inf'):\n",
        "        for k_i in smoke_starts.keys(): # go thru all --> if > start, set it.\n",
        "            if k_i > rangeStart:\n",
        "                # print(\"setting male\", smoke_starts[k_i], \"to\", (smoke_percent_male[smoke_starts[k_i]] + maleSmokingRates[k]) / 2,\n",
        "                #           \"using\", smoke_percent_male[smoke_starts[k_i]], \"and\", maleSmokingRates[k])\n",
        "                smoke_percent_male[smoke_starts[k_i]] = (\n",
        "                    smoke_percent_male[smoke_starts[k_i]] + maleSmokingRates[k]\n",
        "                ) / 2\n",
        "        break\n",
        "    for i in range(rangeStart, rangeEnd):\n",
        "        if i in smoke_starts:\n",
        "            # print(\"setting male\", smoke_starts[i], \"to\", (smoke_percent_male[smoke_starts[i]] + maleSmokingRates[k]) / 2,\n",
        "            #       \"using\", smoke_percent_male[smoke_starts[i]], \"and\", maleSmokingRates[k])\n",
        "            smoke_percent_male[smoke_starts[i]] = (\n",
        "                smoke_percent_male[smoke_starts[i]] + maleSmokingRates[k]\n",
        "            ) / 2\n",
        "\n",
        "for k in ageDict.keys():\n",
        "    rangeStart = k[0]\n",
        "    rangeEnd = k[1]\n",
        "    if rangeEnd == float('inf'):\n",
        "        for k_i in smoke_starts.keys(): # go thru all --> if > start, set it.\n",
        "            if k_i > rangeStart:\n",
        "                # print(\"setting female\", smoke_starts[k_i], \"to\", (smoke_percent_female[smoke_starts[k_i]] + ageDict[k]) / 2,\n",
        "                #           \"using\", smoke_percent_female[smoke_starts[k_i]], \"and\", ageDict[k])\n",
        "                smoke_percent_female[smoke_starts[k_i]] = (\n",
        "                    smoke_percent_female[smoke_starts[k_i]] + ageDict[k]\n",
        "                ) / 2\n",
        "        break\n",
        "    for i in range(rangeStart, rangeEnd):\n",
        "        if i in smoke_starts:\n",
        "            # print(\"setting female\", smoke_starts[i], \"to\", (smoke_percent_female[smoke_starts[i]] + ageDict[k]) / 2,\n",
        "            #       \"using\", smoke_percent_female[smoke_starts[i]], \"and\", ageDict[k])\n",
        "            smoke_percent_female[smoke_starts[i]] = (\n",
        "                smoke_percent_female[smoke_starts[i]] + ageDict[k]\n",
        "            ) / 2\n",
        "\n",
        "print(\"Imputing null 'smoke' values with:\")\n",
        "print(\"Age smoke percent male:\",smoke_percent_male)\n",
        "print(\"Age smoke percent female:\",smoke_percent_female)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlY3hcT78NnK",
        "outputId": "abb4a95d-36d2-49d1-9e75-4f85bbc0f172"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputing null 'smoke' values with:\n",
            "Age smoke percent male: {(15, 17): 1.6, (18, 24): 7.087128712871287, (25, 34): 13.621287128712872, (35, 44): 13.621287128712872, (45, 54): 16.562871287128715, (55, 64): 17.112871287128712, (65, 74): 8.7, (75, inf): 6.832673267326734}\n",
            "Age smoke percent female: {(15, 17): 1.6, (18, 24): 6.3, (25, 34): 11.75, (35, 44): 11.75, (45, 54): 14.350000000000001, (55, 64): 14.9, (65, 74): 8.7, (75, inf): 5.6000000000000005}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find out how many values we are about to impute."
      ],
      "metadata": {
        "id": "spmf8XLZk7h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of null values in the 'smoke' column\n",
        "null_vals_count = df.filter(isnull(col('smoke'))).count()\n",
        "print(\"Number of null values in 'smoke' column:\", null_vals_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOqjsNYEKPTI",
        "outputId": "de50f02b-e944-4ebf-b164-671f2665a82b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null values in 'smoke' column: 669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impute the null values in the 'smoke' column."
      ],
      "metadata": {
        "id": "AnCodY-_k-RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to impute null values in 'smoke' column based on age and sex\n",
        "\n",
        "# Function to create a UDF for imputation logic\n",
        "def impute_smoke(age, sex):\n",
        "    # Get corresponding age smoke percent based on sex\n",
        "    age_smoke_percent = smoke_percent_male if sex == 1 else smoke_percent_female\n",
        "    # Get random percentage for imputation\n",
        "    random_percentage = rd.uniform(0, 100)\n",
        "    # If age falls within any range, impute with the corresponding percentage\n",
        "    for age_range, percent in age_smoke_percent.items():\n",
        "        if age >= age_range[0] and age <= age_range[1]:\n",
        "            return 1 if random_percentage <= percent else 0\n",
        "    return None  # Return None if no age range matches\n",
        "\n",
        "# Register UDF\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "impute_smoke_udf = udf(impute_smoke, IntegerType())\n",
        "\n",
        "# Create a new column 'imputed_smoke' with imputed values\n",
        "df = df.withColumn(\"imputed_smoke\",\n",
        "                   when(col(\"smoke\").isNull(), impute_smoke_udf(col(\"age\"), col(\"sex\")))\n",
        "                   .otherwise(col(\"smoke\")))\n",
        "\n",
        "# Drop the old 'smoke' column and rename 'imputed_smoke' to 'smoke'\n",
        "df = df.drop(\"smoke\")\n",
        "df = df.withColumnRenamed(\"imputed_smoke\", \"smoke\")\n",
        "\n",
        "print(\"Finished imputing values in the 'smoke' column.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNN2A8cUsEBy",
        "outputId": "34a37d99-9115-46d2-e69b-a02988687427"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished imputing values in the 'smoke' column.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double-check that we have imputed all values in the smoke column."
      ],
      "metadata": {
        "id": "5HtPrWX1lAmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of null values in the 'smoke' column\n",
        "null_vals_count = df.filter(isnull(col('smoke'))).count()\n",
        "print(\"Number of null values in 'smoke' column:\", null_vals_count)\n",
        "\n",
        "\n",
        "print(\"After imputing data:\")\n",
        "df.select('age', 'sex', 'smoke', 'abs_smoke', 'cdc_smoke').show(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-VBJI3jubs7",
        "outputId": "70b5e815-83aa-49be-b162-39ef84d2869f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null values in 'smoke' column: 0\n",
            "After imputing data:\n",
            "+---+---+-----+---------+---------+\n",
            "|age|sex|smoke|abs_smoke|cdc_smoke|\n",
            "+---+---+-----+---------+---------+\n",
            "| 63|  1|    0|        0|        0|\n",
            "| 67|  1|    0|        0|        0|\n",
            "| 67|  1|    0|        0|        0|\n",
            "| 37|  1|    0|        0|        0|\n",
            "| 41|  0|    0|        0|        0|\n",
            "| 56|  1|    0|        1|        0|\n",
            "| 62|  0|    0|        1|        0|\n",
            "| 57|  0|    0|        1|        0|\n",
            "| 63|  1|    1|        0|        0|\n",
            "| 53|  1|    0|        0|        0|\n",
            "| 57|  1|    1|        0|        0|\n",
            "| 56|  0|    0|        1|        0|\n",
            "| 56|  1|    0|        0|        0|\n",
            "| 44|  1|    0|        0|        0|\n",
            "| 52|  1|    1|        0|        0|\n",
            "| 57|  1|    0|        0|        1|\n",
            "| 48|  1|    1|        0|        0|\n",
            "| 54|  1|    1|        0|        0|\n",
            "| 48|  0|    0|        1|        0|\n",
            "| 49|  1|    0|        0|        1|\n",
            "+---+---+-----+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the percentage of male/female smokers in different age buckets based on the combined data sources that we have just randomly generated to impute the 'smoke' column. Also plot the expected percentage of male/female smokers for each age group based on the combined data percentages."
      ],
      "metadata": {
        "id": "LPN9gHKOlHTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # plot smoke values again\n",
        "\n",
        "# age_buckets = list(smoke_percent_male.keys())\n",
        "\n",
        "\n",
        "# # Plotting\n",
        "# fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# # Initialize lists to store percentage of male and female smokers\n",
        "# male_percentages = []\n",
        "# female_percentages = []\n",
        "\n",
        "# # Iterate over age buckets\n",
        "# for bucket in age_buckets:\n",
        "#     # Filter data for the current age bucket\n",
        "#     age_min, age_max = bucket\n",
        "#     age_filtered = df[(df['age'] >= age_min) & (df['age'] <= age_max)]\n",
        "\n",
        "#     # Calculate percentage of male and female smokers\n",
        "#     male_count = age_filtered[(age_filtered['sex'] == 1) & (age_filtered['smoke'] == 1)].shape[0]\n",
        "#     female_count = age_filtered[(age_filtered['sex'] == 0) & (age_filtered['smoke'] == 1)].shape[0]\n",
        "#     total_count_male = age_filtered[age_filtered['sex'] == 1].shape[0]\n",
        "#     total_count_female = age_filtered[age_filtered['sex'] == 0].shape[0]\n",
        "\n",
        "#     # Calculate percentages\n",
        "#     male_percentage = (male_count / total_count_male) * 100 if total_count_male > 0 else 0\n",
        "#     female_percentage = (female_count / total_count_female) * 100 if total_count_female > 0 else 0\n",
        "\n",
        "#     # Append to lists\n",
        "#     male_percentages.append(male_percentage)\n",
        "#     female_percentages.append(female_percentage)\n",
        "\n",
        "# # Plotting bar plot\n",
        "# bar_width = 0.15\n",
        "# index = range(len(age_buckets))\n",
        "\n",
        "# ax.bar([i - bar_width for i in index], male_percentages, bar_width, label='Male', color='blue', alpha=0.66)\n",
        "# ax.bar([i for i in index], list(smoke_percent_male.values()), bar_width, label='Expected male', color='blue', alpha=0.3)\n",
        "# ax.bar([i + bar_width for i in index], female_percentages, bar_width, label='Female', color='red', alpha=0.66)\n",
        "# ax.bar([i + 2*bar_width for i in index], list(smoke_percent_female.values()), bar_width, label='Expected female', color='red', alpha=0.3)\n",
        "\n",
        "# # Set labels and title\n",
        "# ax.set_xlabel('Age Group')\n",
        "# ax.set_ylabel('Percentage of Smokers')\n",
        "# ax.set_title('Percentage of Male and Female Smokers by Age Group (imputed)')\n",
        "# ax.set_xticks([i + bar_width / 2 for i in index])\n",
        "# ax.set_xticklabels([f'{bucket[0]}-{bucket[1]}' for bucket in age_buckets])\n",
        "# ax.legend()\n",
        "\n",
        "# # Show plot\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "n7CTzHbpvNsa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure there are no remaining null values before training our models."
      ],
      "metadata": {
        "id": "VcPFuFn5ljts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of null values in each column\n",
        "total_count = df.count()\n",
        "null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
        "null_counts = null_counts.collect()[0].asDict()\n",
        "\n",
        "null_percentages = {k: (v / total_count) * 100 for k, v in null_counts.items()}\n",
        "null_percentages_sorted = dict(sorted(null_percentages.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Print each key-value pair in sorted order\n",
        "print(\"Percentage of null values in each column:\")\n",
        "for column, percentage in null_percentages_sorted.items():\n",
        "    print(f\"{column}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XJQEGJS7ou2",
        "outputId": "087acca2-a1e5-49fd-d29c-4d47182fb190"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of null values in each column:\n",
            "age: 0.00%\n",
            "sex: 0.00%\n",
            "painloc: 0.00%\n",
            "painexer: 0.00%\n",
            "cp: 0.00%\n",
            "trestbps: 0.00%\n",
            "fbs: 0.00%\n",
            "prop: 0.00%\n",
            "nitr: 0.00%\n",
            "pro: 0.00%\n",
            "diuretic: 0.00%\n",
            "thaldur: 0.00%\n",
            "thalach: 0.00%\n",
            "exang: 0.00%\n",
            "oldpeak: 0.00%\n",
            "slope: 0.00%\n",
            "target: 0.00%\n",
            "abs_smoke: 0.00%\n",
            "cdc_smoke: 0.00%\n",
            "smoke: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Training the classification models using the imputed data."
      ],
      "metadata": {
        "id": "RperK3vHlzWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into test and training sets, and plot the distribution of the 'target' column (the one we're trying to classify) to ensure that it is evenly stratified between test and training sets."
      ],
      "metadata": {
        "id": "6xTn7eOCl5yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the describe() function to generate summary statistics\n",
        "summary = df.describe()\n",
        "\n",
        "# Show the summary\n",
        "summary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkLphWaDWY_x",
        "outputId": "9c8a9f2f-45e5-4bd8-e1e1-33b0e3a3459b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "|summary|               age|                sex|            painloc|           painexer|                cp|          trestbps|                fbs|               prop|               nitr|                pro|           diuretic|           thaldur|           thalach|              exang|           oldpeak|             slope|            target|         abs_smoke|         cdc_smoke|              smoke|\n",
            "+-------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "|  count|               899|                899|                899|                899|               899|               899|                899|                899|                899|                899|                899|               899|               899|                899|               899|               899|               899|               899|               899|                899|\n",
            "|   mean|53.480533926585096| 0.7908787541713015| 0.9454949944382648| 0.7208008898776418| 3.253615127919911|132.75305895439377|0.15016685205784205|0.23804226918798665|0.24694104560622915|0.16017797552836485|0.10233592880978866| 8.655871916135506|137.28031145717463| 0.3670745272525028|0.8732926827897882| 1.846496106785317|0.5506117908787542|0.1568409343715239|0.1724137931034483|0.23581757508342602|\n",
            "| stddev| 9.435893735264402|0.40690751506650724|0.22713783852996763|0.44885529158423826|0.9284993705475486|17.401750260378485|0.35743374325230937|0.42612219771712523|0.43147218833254275|0.36697519308924714| 0.3032582795750937|3.6279159885262646|25.158326306234056|0.48227537671224074|0.9649913773010523|0.5156962369824244| 0.497708740681981|  0.36385315488877|0.3779499590805692|0.42474500942510135|\n",
            "|    min|                28|                  0|                  0|                  0|                 1|               100|                  0|                  0|                  0|                  0|                  0|               1.0|                60|                  0|               0.0|                 0|                 0|                 0|                 0|                  0|\n",
            "|    max|                77|                  1|                  1|                  1|                 4|               200|                  1|                  1|                  1|                  1|                  1|              24.0|               202|                  1|               4.0|                 3|                 1|                 1|                 1|                  1|\n",
            "+-------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and test using a 90-10 training test split with stratification on\n",
        "# the labels (i.e., both sets contain roughly the same proportion of positive labels).\n",
        "\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "# X = df.drop(columns=['target'])  # Features (all except target)\n",
        "# y = df['target']  # Target variable\n",
        "\n",
        "# # Split the data into training and test sets with stratification\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
        "\n",
        "# # Check the shapes of the split sets\n",
        "# print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "# print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col, lit, monotonically_increasing_id\n",
        "# Add unique identifier to each row\n",
        "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "# stratify dataset on target.\n",
        "fractions = df.select(\"target\").distinct().withColumn(\"fraction\", lit(0.9)).rdd.collectAsMap()\n",
        "train_set = df.stat.sampleBy(\"target\", fractions, 40)\n",
        "\n",
        "df_total = df.count()\n",
        "\n",
        "df_target_1_count = df.filter(col(\"target\") == 1).count()\n",
        "df_target_1_percentage = (df_target_1_count / df_total) * 100\n",
        "print(f\"Percentage of target=1 in the dataset: {df_target_1_percentage:.2f}%\")\n",
        "\n",
        "# Create the test set by excluding the training set\n",
        "# test_set = df.subtract(train_set)\n",
        "test_set = df.join(train_set, on=\"id\", how=\"left_anti\")\n",
        "\n",
        "\n",
        "train_total = train_set.count()\n",
        "test_total = test_set.count()\n",
        "# Print the counts of the split sets\n",
        "print(\"Training set count:\", train_total, f\"({(train_total/df_total * 100):.2f}%)\")\n",
        "print(\"Test set count:\", test_total, f\"({(test_total/df_total * 100):.2f}%)\")\n",
        "print(\"Total number of entries:\", df_total)\n",
        "\n",
        "# Show the distribution of the target variable in the training set\n",
        "print(\"\\nTrain set:\")\n",
        "train_set.groupBy(\"target\").count().show()\n",
        "train_target_1_count = train_set.filter(col(\"target\") == 1).count()\n",
        "train_target_1_percentage = (train_target_1_count / train_total) * 100\n",
        "print(f\"Percentage of target=1 in the training dataset: {train_target_1_percentage:.2f}%\")\n",
        "\n",
        "# Show the distribution of the target variable in the test set\n",
        "print(\"\\nTest set:\")\n",
        "test_set.groupBy(\"target\").count().show()\n",
        "test_target_1_count = test_set.filter(col(\"target\") == 1).count()\n",
        "test_target_1_percentage = (test_target_1_count / test_total) * 100\n",
        "print(f\"Percentage of target=1 in the testing dataset: {test_target_1_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NazysSZu4iJA",
        "outputId": "a9ec2ec7-de85-4d97-e143-1f6448258edf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of target=1 in the dataset: 55.06%\n",
            "Training set count: 819 (91.10%)\n",
            "Test set count: 80 (8.90%)\n",
            "Total number of entries: 899\n",
            "\n",
            "Train set:\n",
            "+------+-----+\n",
            "|target|count|\n",
            "+------+-----+\n",
            "|     0|  368|\n",
            "|     1|  451|\n",
            "+------+-----+\n",
            "\n",
            "Percentage of target=1 in the training dataset: 55.07%\n",
            "\n",
            "Test set:\n",
            "+------+-----+\n",
            "|target|count|\n",
            "+------+-----+\n",
            "|     1|   44|\n",
            "|     0|   36|\n",
            "+------+-----+\n",
            "\n",
            "Percentage of target=1 in the testing dataset: 55.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the binary classification models on the training data, tune hyperparameters, and assess performance using 5-fold cross-validation."
      ],
      "metadata": {
        "id": "n3ykr0yHmEhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a few binary classification models on the training data. If there are\n",
        "# hyperparameters, tune them appropriately. Assess the performance using 5-fold cross-\n",
        "# validation and report relevant performance metrics.\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Define a Random Forest classifier\n",
        "# classifier = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
        "\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(featuresCol=\"features\", labelCol=\"target\"),\n",
        "    'Random Forest': RandomForestClassifier(featuresCol=\"features\", labelCol=\"target\"),\n",
        "    'SVM': LinearSVC(featuresCol=\"features\", labelCol=\"target\")\n",
        "}\n",
        "\n",
        "# Define parameter grids\n",
        "param_grids = {\n",
        "    'Logistic Regression': {},\n",
        "    'Random Forest': {'numTrees': [50, 100, 150]},\n",
        "    'SVM': {'maxIter': [10, 50, 100]}\n",
        "}\n",
        "\n",
        "best_model_name = None\n",
        "highest_cv = 0\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Define pipeline\n",
        "    pipeline = Pipeline(stages=[model])\n",
        "\n",
        "    # Define parameter grid\n",
        "    param_grid = ParamGridBuilder().addGrid(model, param_grids[model_name]).build()\n",
        "\n",
        "    # Define cross-validator\n",
        "    cross_validator = CrossValidator(estimator=pipeline,\n",
        "                                     estimatorParamMaps=param_grid,\n",
        "                                     evaluator=BinaryClassificationEvaluator(),\n",
        "                                     numFolds=5)\n",
        "\n",
        "    # Fit cross-validator to train set\n",
        "    cv_model = cross_validator.fit(train_set)\n",
        "\n",
        "    # Make predictions on test set\n",
        "    predictions = cv_model.transform(test_set)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"ROC AUC score: {auc:.4f}\")\n",
        "    print(\"-------------------------------------\")\n",
        "\n",
        "    # Update best model\n",
        "    if auc > highest_cv:\n",
        "        best_model_name = model_name\n",
        "        highest_cv = auc\n",
        "\n",
        "# Print best model\n",
        "print(f\"Best model: {best_model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "hlsBEUAbNFuc",
        "outputId": "fc98cc05-6ee8-4e9a-bd64-44517e17c81e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "__provides__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-9e3e57eeae4b>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Define models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m models = {\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;34m'Logistic Regression'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;34m'Random Forest'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m'SVM'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, threshold, thresholds, probabilityCol, rawPredictionCol, standardization, weightCol, aggregationDepth, family, lowerBoundsOnCoefficients, upperBoundsOnCoefficients, lowerBoundsOnIntercepts, upperBoundsOnIntercepts, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0mParams\u001b[0m \u001b[0mare\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mequivalent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \"\"\"\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m         self._java_obj = self._new_java_obj(\n\u001b[1;32m   1319\u001b[0m             \u001b[0;34m\"org.apache.spark.ml.classification.LogisticRegression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"JavaObject\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJavaWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LogisticRegressionParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m         self._setDefault(\n\u001b[1;32m   1010\u001b[0m             \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxBlockSizeInMB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasProbabilityCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilityCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasThresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetThresholds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasRawPredictionCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawPredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawPrediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasLabelCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasFeaturesCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasPredictionCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasRegParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetRegParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasElasticNetParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melasticNetParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasMaxIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetMaxIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasFitIntercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitIntercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasTol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetTol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasStandardization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasWeightCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetWeightCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasAggregationDepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregationDepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasThreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/shared.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasMaxBlockSizeInMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxBlockSizeInMB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Copy the params from the class to the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_copy_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_copy_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \"\"\"\n\u001b[1;32m    275\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0msrc_name_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0msrc_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mnameAttr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnameAttr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_name_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \"\"\"\n\u001b[1;32m    275\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0msrc_name_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0msrc_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mnameAttr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnameAttr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_name_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: __provides__"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the model with the highest 5-fold cross-validation."
      ],
      "metadata": {
        "id": "L1FvFycAI2TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The model with the highest 5-fold cross-validation (at\", str(highest_cv) + \") was\",best_model_name + \".\")"
      ],
      "metadata": {
        "id": "gqW21345Mpu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Saving the imputed data back to the S3 bucket"
      ],
      "metadata": {
        "id": "wBewHp4kmLUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataframe back to the s3 bucket as a CSV file.\n",
        "\n",
        "# you need to change the credentials for yourself\n",
        "\n",
        "bucket_name = 'brennanbensonawsbucket'\n",
        "object_key = 'data/heart_disease/'\n",
        "\n",
        "s3 = boto3.client('s3',\n",
        "                      aws_access_key_id=\"ASIAYAAO5HRMG2ADFJ7L\",\n",
        "                      aws_secret_access_key=\"QpFo4m94dxap5OttjkI2w43iIeo9ta0xL05uOf4H\",\n",
        "                      aws_session_token=\"IQoJb3JpZ2luX2VjEBgaCXVzLWVhc3QtMiJHMEUCIQCOdYHSG3VNmaSoWENqnJmqUVauqBWVaablFsbEqnuLDwIgP5vUT/HR1R4ckzjj0pYas2T4DIKwj3DJ+REn5UrKsQUq9AIIkf//////////ARAAGgw1NDk3ODcwOTAwMDgiDN2Eb46CE/3fcHfkwCrIAtO8MKZqpp33DGMuMR0m9+pZdugVANMa+h2FjG9b+OOvjYzC00O9NSpUicz4Y1cttE+vsws3RqoNHAEfO/37Iu/4m2tSk3fNWVEu82LaKKQnOKBelUAwTgvQe8TZ51LAlCGgRDILMrf9Unov/nZT/U/xLfIRm0bZeiOhAtX2N5k1Lepcn/sGzlvBLdu4j9zSpt6mKYeXU68dYvnvZ3EnDdeCjrBYJiKTgoeXilAMf7mwJmg2XwAp225Hyw6nBlaJ2/j8xxxBpPzK7eHi9DIUQ/kf0hCvZOXFyduonER8HAROqpkoqNikBCL/MGcvmaqM0sVGPu1KMI5xRkW07YlLacspU+DRc1ZfWYt+rMzQUxhNJrWdztiRZ39CiL0fpKZhJFTz0kjxHpsQ8d8C5lwaY+GarTNY8+1DFvGkXyuVyT7U037wRAT0/aswqvDCsgY6pwGxmWcE6TcIvkax7vmRkOz08eM1fzKjwb8UL65ghGCkJsMokb7hRWN37tvYlR/sjjs1wYM8YpVRGbRq2RcgQ6Fy5IPmkJ3QzF0cl575gE7R3oxw+QOcX4nsHn6TF+lBCFYaZ/uZOhyImstjXlKAr6lM0y/eyR0mEdzMYqWS09BFGI3U2VnBXevpoOBRl4m0gXRNwyUGkGN8XzD3s9t3tkgL/sxOTWju2g==\"\n",
        "                      )\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "df.repartition(1).write.csv(\"heart_disease_cleaned\", mode=\"overwrite\")\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Get the current directory\n",
        "current_directory = os.getcwd() + \"/heart_disease_cleaned\"\n",
        "\n",
        "# List all files in the current directory\n",
        "files = os.listdir(current_directory)\n",
        "\n",
        "# Filter CSV files\n",
        "csv_files = [file for file in files if file.endswith('.csv')]\n",
        "\n",
        "# Upload each CSV file to S3\n",
        "for csv_file in csv_files:\n",
        "    local_file_path = os.path.join(current_directory, csv_file)\n",
        "    object_key = f'data/heart_disease_cleaned/{csv_file}'  # Change this to your desired S3 directory structure\n",
        "    try:\n",
        "        s3.upload_file(local_file_path, bucket_name, object_key)\n",
        "        print(f\"File '{csv_file}' uploaded successfully to s3://{bucket_name}/{object_key}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file '{csv_file}' to S3: {e}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i22xGDEmI1sd",
        "outputId": "5ff41b3f-8790-4e1c-889d-6df544ad91af"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'part-00000-7838f101-21dd-4f4e-9ed9-db9ffa8b843e-c000.csv' uploaded successfully to s3://brennanbensonawsbucket/data/heart_disease_cleaned/part-00000-7838f101-21dd-4f4e-9ed9-db9ffa8b843e-c000.csv\n"
          ]
        }
      ]
    }
  ]
}